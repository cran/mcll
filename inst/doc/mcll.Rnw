%% bold roman letters
\def\AA{\mbox{$\mathbf A$}}
\def\aa{\mbox{$\mathbf a$}}
\def\ao{\mbox{\aa}}
\newcommand{\BB}{\mbox{$\mathbf B$}}
\newcommand{\bb}{\mbox{$\mathbf b$}}
\newcommand{\CC}{\mbox{$\mathbf C$}}
\newcommand{\dd}{\mbox{$\mathbf d$}}
\newcommand{\DD}{\mbox{$\mathbf D$}}
\newcommand{\EE}{\mbox{$\mathbf E$}}
\newcommand{\ee}{\mbox{$\mathbf e$}}
\newcommand{\FF}{\mbox{$\mathbf F$}}
\newcommand{\ff}{\mbox{$\mathbf f$}}
\def\GG{\mbox{$\mathbf G$}}
\def\gg{\mbox{$\mathbf g$}}
\newcommand{\HH}{\mbox{$\mathbf H$}}
\newcommand{\hh}{\mbox{$\mathbf h$}}
\def\II{\mbox{$\mathbf I$}}
\newcommand{\KK}{\mbox{$\mathbf K$}}
\newcommand{\lv}{\mbox{$\mathbf l$}}
\def\LL{\mbox{$\mathbf L$}}
\def\MM{\mbox{$\mathbf M$}}
\newcommand{\mm}{\mbox{$\mathbf m$}}
\def\NN{\mbox{$\mathbf N$}}
\newcommand{\one}{\mbox{$\mathbf 1$}}
\newcommand{\pp}{\mbox{$\mathbf p$}}
\newcommand{\QQ}{\mbox{$\mathbf Q$}}
\def\rr{\mbox{$\mathbf r$}}
\newcommand{\RR}{\mbox{$\mathbf R$}}
\def\SS{\mbox{$\mathbf S$}}
\def\TT{\mbox{$\mathbf T$}}
\def\sb{\mbox{$\mathbf s$}}
\def\uu{\mbox{$\mathbf u$}}
\newcommand{\UU}{\mbox{$\mathbf U$}}
\newcommand{\VV}{\mbox{$\mathbf V$}}
\def\vv{\mbox{$\mathbf v$}}
\def\ww{\mbox{$\mathbf w$}}
\def\WW{\mbox{$\mathbf W$}}
\def\XX{\mbox{$\mathbf X$}}
\def\xx{\mbox{$\mathbf x$}}
\newcommand{\YY}{\mbox{$\mathbf Y$}}
\newcommand{\yy}{\mbox{$\mathbf y$}}
\newcommand{\ZZ}{\mbox{$\mathbf Z$}}
\newcommand{\zz}{\mbox{$\mathbf z$}}


%% bold numbers
\newcommand{\zed}{\mbox{$\mathbf z$}}
\newcommand{\0}{\mbox{$\mathbf 0$}}
\newcommand{\nul}{\mbox{$\mathbf 0$}}
\def\one{\mbox{$\mathbf 1$}}
\def\zer{\mbox{$\mathbf 0$}}


%% bold Greek letters
\newcommand{\al}{\mbox{\boldmath $\alpha$}}
\newcommand{\be}{\mbox{\boldmath $\beta$}}
\newcommand{\DE}{\mbox{\boldmath $\Delta$}}
\newcommand{\de}{\mbox{\boldmath $\delta$}}
\newcommand{\et}{\mbox{\boldmath $\eta$}}
\newcommand{\ep}{\mbox{\boldmath $\epsilon$}}
\newcommand{\GA}{\mbox{\boldmath $\Gamma$}}
\newcommand{\ga}{\mbox{\boldmath $\gamma$}}
\newcommand{\io}{\mbox{\boldmath $\iota$}}
\newcommand{\ka}{\mbox{\boldmath $\kappa$}}
\newcommand{\LA}{\mbox{\boldmath $\Lambda$}}
\newcommand{\la}{\mbox{\boldmath $\lambda$}}
\def\muu{\mbox{\boldmath $\mu$}}
\newcommand{\nuu}{\mbox{$\bm{\nu}$}}
\newcommand{\OM}{\mbox{\boldmath $\Omega$}}
\newcommand{\om}{\mbox{\boldmath $\omega$}}
\newcommand{\PH}{\mbox{\boldmath $\Phi$}}
\newcommand{\ph}{\mbox{\boldmath $\phi$}}
\newcommand{\PII}{\mbox{\boldmath $\Pi$}}
\newcommand{\pii}{\mbox{\boldmath $\pi$}}
\newcommand{\PS}{\mbox{\boldmath $\Psi$}}
\newcommand{\ps}{\mbox{\boldmath $\psi$}}
\newcommand{\si}{\mbox{\boldmath $\sigma$}}
\newcommand{\SI}{\mbox{\boldmath $\Sigma$}}
\newcommand{\ta}{\mbox{\boldmath $\tau$}}
\def\th{\mbox{\boldmath $\theta$}}
\def\TH{\mbox{\boldmath $\Theta$}}
\newcommand{\xii}{\mbox{\boldmath $\xi$}}
\newcommand{\XII}{\mbox{\boldmath $\Xi$}}
\newcommand{\ze}{\mbox{\boldmath $\zeta$}}
\newcommand{\vart}{\mbox{\boldmath $\vartheta$}}
\newcommand{\varp}{\mbox{\boldmath $\varphi$}}
\newcommand{\vars}{\mbox{\boldmath $\varsigma$}}
\newcommand{\varr}{\mbox{\boldmath $\varrho$}}


%% roman cov, etc.
\def\cor{\mbox{\rm Cor}} % don't use this
\def\mean{\mbox{\rm mean}}
\def\var{\mbox{\rm Var}}
\def\Var{\mbox{\rm Var}}
\def\cov{\mbox{\rm Cov}}
\def\Cov{\mbox{\rm Cov}}
\def\Cor{\mbox{\rm Cor}}
\def\Pr{\mbox{\rm Pr}}
\def\E{\mbox{\rm E}}
%\def\E{\mbox{$E$}}
\def\trace{\mbox{\rm trace}}
\def\ln{\mbox{\rm ln}}
\def\logit{\mbox{\rm logit}}
\def\se{\mbox{\rm se}}
\def\SE{\mbox{\rm SE}}
\def\No{\mbox{\rm N}}

\documentclass[11pt]{article}
\textheight 8.5in \textwidth 6.5in \topmargin 0.0in \headheight
0.0in \oddsidemargin  .5in \evensidemargin .5in \hoffset=-.5in

%\usepackage[authoryear,round]{natbib}
\usepackage{rotating,epsfig}
\usepackage{amssymb,amsmath}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{setspace}
\usepackage{psfrag}
\usepackage{lscape}


%\VignetteIndexEntry{R package mcll for Monte Carlo local likelihood estimation}
%\VignetteDepends{mcll}
%\VignetteKeywords{Generalized linear mixed models; Crossed random effects; local likelihood density estimation}
%\VignettePackage{mcll}

\SweaveOpts{keep.source=TRUE}
<<echo=FALSE, results=hide>>=
library("mcll")
options(prompt = "R> ", continue = "+  ", width = 70,
  digits = 4, show.signif.stars = FALSE, useFancyQuotes = FALSE)
@


\begin{document}



\title{R package \texttt{mcll} for \\
Monte Carlo local likelihood estimation}
\author{Minjeong Jeon \\ University of California, Berkeley
        \and  Cari Kaufman \\ University of California, Berkeley
        \and Sophia Rabe-Hesketh \\ University of California, Berkeley}


\maketitle



\begin{abstract}

\noindent \texttt{mcll} is an R package for Monte Carlo local likelihood (MCLL) estimation of generalized linear mixed models with crossed random 
effects. \texttt{mcll} implements the nested maximizations in the MCLL algorithm (Step 2),
given posterior samples of model parameters for a particular set of priors (Step 1).
\texttt{mcll} also provides standard error estimates for the MCLL parameter estimates.
This paper describes how the MCLL algorithm works with the package \texttt{mcll}.
The widely-used salamander mating data are used for illustration.
\end{abstract}



\section{Monte Carlo Local Likelihood Method}

Monte Carlo local likelihood (MCLL) is an approximate maximum likelihood method for estimating generalized linear mixed models (GLMM)
with crossed random effects.
MCLL initially treats model parameters as random variables and samples them from the posterior for a particular prior.
The likelihood function is approximated up to a constant by fitting a density to the posterior samples and dividing it by the prior.
The posterior density is approximated using local likelihood density estimation (Loader, 1996), %\citep{loader:96},
where the log-likelihood is locally approximated by a polynomial function.
For details on MCLL, see Jeon et al. (2012). %\citet{jeon:12c}.

Here we describe the  procedure of MCLL for parameter estimation.
Specifically, assuming a $d$-dimensional parameter space $\boldsymbol{\theta}$  with observed data vector $\yy$,
the MCLL algorithm involves the following two steps:

\begin{description}

\item [Step 1] Choose a prior $p (\boldsymbol{\theta})$  and use a MCMC method to obtain samples from the posterior $ p(\theta | \yy)$
\begin{align*}
p (\boldsymbol{\theta} | \yy) = \frac{L(\yy|\boldsymbol{\theta}) p (\boldsymbol{\theta})} {C_s}, %\label{eq:loc2}
\end{align*}
\noindent where the normalizing constant is $C_s = \int L(\yy| \boldsymbol{\theta}) p (\boldsymbol{\theta}) d \boldsymbol{\theta} $. %$L(\theta)$ 
is the likelihood and

%Obtain Markov chain Monte Carlo (MCMC) samples of model parameters from the posterior for a particular prior


\item [Step 2] Maximize an approximation to the likelihood defined up to constant $C_s$ by
\begin{equation*} %\label{eq:locunweight}
\hat{L}(\yy | \boldsymbol{\theta}) = \frac{1} {p(\boldsymbol{\theta})} \text{Ps}_p (\boldsymbol{\theta} ), %\frac{1} {\cdot p(\theta)} \text{Ps}_p 
(\theta),
\end{equation*}
\noindent where % $ \{\theta^{(j)} \}_{j=1}^{m}$ are the sample points and
$\text{Ps}_p (\boldsymbol{\theta})$ is the local likelihood estimate of the posterior density.
Specifically, for a given value of $\boldsymbol{\theta}$, this is obtained by assuming that the log-posterior density can be
approximated by a polynomial function $P_{\boldsymbol{a}}(\uu-\boldsymbol{\theta})$ %= a_0 + a_1(u-\theta) + \cdots + a_p(u-\theta)^p$ of degree 
$p$
with parameters $\boldsymbol{a}$. %= (a_0, a_1, ..., a_p)'$.
For example, in the three dimensional case ($d$=3), the log-posterior can be locally approximated by a quadratic function
\begin{align*}
\nonumber  P_{\boldsymbol{a}}(\uu-\boldsymbol{\theta}) &= a_0 + a_1 (u_1 - \theta_1) + a_2 (u_2- \theta_2 ) + a_3 (u_3 - \theta_3 ) \\
\nonumber        & + \frac{1}{2}a_4 (u_1 - \theta_1 )^2 + \frac{1}{2}a_5 (u_2 - \theta_2 )^2 + \frac{1}{2}a_6 (u_3 - \theta_3 )^2 \\
\nonumber        & + a_7 (u_1 - \theta_1) (u_2 - \theta_2 ) + a_8 (u_1- \theta_1)(u_3 - \theta_3) \\
       & + a_9 (u_2 - \theta_2 )(u_3 - \theta_3 ), %\label{eq:quad}
\end{align*}
\noindent where  $\boldsymbol{a} = (a_0, a_1, ..., a_9)'$.


The $\boldsymbol{a}$ parameters are estimated for a particular $\boldsymbol{\theta}$
by maximizing a localized version of the log-likelihood, which in this case is %with respect to $\boldsymbol{a}$
\begin{align}
 \hat{l} (\boldsymbol{\theta}, \boldsymbol{a} ) = \sum_{j=1}^{m} K \left(\frac{\boldsymbol{\theta}^{(j)}
 -\boldsymbol{\theta}}{\hh}\right) P_{\boldsymbol{a}}(\boldsymbol{\theta}^{(j)} - \boldsymbol{\theta})
            - m \int K \left(\frac{\uu -\boldsymbol{\theta}}{\hh}\right) \exp{(P_{\boldsymbol{a}}(\uu-\boldsymbol{\theta}))}  d \uu ,
\label{eq:loc2}
\end{align}
\noindent where $ \{\boldsymbol{\theta}^{(j)} \}_{j=1}^{m}$ are the posterior sample points. % and $m$ is the number of the samples.
%We call the estimated density $\text{Ps}_p(\boldsymbol{\theta})$.

\end{description}

The MCLL method also provides a relatively simple way to compute standard errors.
Specifically, we derive an alternative way of computing the Hessian matrix for MCLL
by using the quadratic approximation of the log-posterior obtained using local likelihood density estimation,
assuming the log-posterior can be well approximated by a quadratic polynomial in the neighborhood of the mode.
For more details, see Section 3 in Jeon et al. (2012). %\citet{jeon:12c}.

The package \texttt{mcll} consists of two main functions, \texttt{mcll\_est} and \texttt{mcll\_se}.
\texttt{mcll\_est} implements Step 2 in the MCLL procedure above, given the posterior samples obtained for a particular prior (in Step 1).
It requires the values and posterior samples of model parameters on the real line.
For example, log transformation of variance parameters is needed.
%We will explain how these transformations can be done in Section 2.
\texttt{mcll\_se} computes standard errors for the MCLL parameter estimates.
In the next section,
we illustrate how to implement the MCLL method (including Step 1) using a real data example.

\section{Illustration}

%Here we illustrate use of \texttt{mcll} using a generalized linear mixed model with crossed random effects.

We use an example of a crossed random effects model using the salamander mating data (McCullagh, 1989, Section 14.5). %\citep[Section 14.5]{mccullagh:89}.
This dataset is a benchmark that has been used to compare many different estimation methods for GLMMs with crossed random effects. 

% \citep[e.g.,][]{ karim:92, breslow:93, booth:99, lee:06, cho:11}.

The salamander mating data consist of three separate experiments, each involving matings among salamanders of two different populations,
called Rough Butt (RB) and White Side (WS).
Sixty females and sixty males of two populations of salamander were paired by a crossed, blocked, and incomplete design in an experiment studying 
whether the two populations have developed generic mechanisms which would prevent inter-breeding.
The response is a binary variable indicating whether mating was successful between female $i$ and male $j$. We adopted model A used by 
Karim and Zeger (1992).
%\citet{karim:92}
\begin{equation}\label{md:sal}
\text{logit}(p (y_{ij} = 1 | z^f_i,  z^m_j)) = \beta_1 + \beta_2 x_{1i} + \beta_3 x_{2j} + \beta_4 x_{1i}x_{2j}  + z^f_i + z^m_j,
\end{equation}
\noindent where %$\xx_{ij}$ is a vector of covariates with regression coefficients $\be$.
the covariates are dummy variables for White Side female ($x_i$), White Side male ($x_j$),
and the interaction ($x_{1i}x_{2j}$).  % $\be = (\beta_{RR}, \beta_{RW}, \beta_{WR}, \beta_{WW})$ and $i,j=1,..,60$;
The two crossed random effects are random intercepts $z^f_i \sim N(0, \sigma^2_f)$ for females and $z^m_j \sim N(0, \sigma^2_m)$ for males.
Each salamander participates in six matings, resulting in 360 matings in total.

Note that the two variance components in model (2) are reparameterized as
$\tau_f = \text{log} \sigma_f $ and $\tau_m = \text{log} \sigma_m$.


\subsection{Step 1: Obtain Posterior Samples}

To obtain posterior samples for parameters, priors should be specified for model parameters.
For model (2), we choose diffuse normal priors
for the fixed effect parameters (with mean 0, standard deviation 100)
and for the log-parameterized standard deviation parameters (with mean -0.98 and standard deviation 0.76).
For details on this choice of priors, see Section 5.1 in Jeon et al. (2012). %\citet{jeon:12c}.

Given the specified priors, the posterior samples can be obtained by any Markov chain Monte Carlo (MCMC) method.
For example, the Bayesian software \texttt{WinBUGS} (Lunn et al., 2000) can be used together with
the R package \texttt{R2WinBUGS} (Sturtz et al., 2005). %\citep{sturtz:05}.
In this example, we use three chains with relatively diffuse starting values.
Each chain was run for 1,000 iterations after a 2,000 iteration burn-in period.
For convergence assessment, the Gelman-Rubin statistic (Gelman and Rubin, 1992) %\citep{gelman:92}
is used in addition to graphical checks such as trace plots and autocorrelation plots.
Here is an example code of using \texttt{R2WinBUGS} to run \texttt{WinBUGS}.

\begin{verbatim}
library(R2WinBUGS)
# dataset
data <- data(salamander)

# set up
n <- nrow(data) # all salamander
m <- length(unique(data$male)) # number of male
f <- length(unique(data$female))  # number of female
r <- data$y  # response (mating)
male <- data$male  # male id
female <- data$female # female id
wsm <- data$wsm  # WSM
wsf <- data$wsf  # WSF
ww <- data$ww  # WW

# mean and sd for the log variable
lmu <- -0.9870405  # mean
lsig <- 1/0.766672   # 1/sd (precision =1/sd^2)

# data set
sala.data <- list("n","m","f","r","male","female","wsm","wsf","ww","lmu","lsig")

# initial value set
sala.inits <- function() {
 list(rm= rnorm(m), rf=rnorm(f), b0 = rnorm(1), b1 = rnorm(1),
      b2 = rnorm(1), b3 = rnorm(1),
      tau0 =rnorm(1,lmu,sqrt(1/lsig)), tau1 =rnorm(1,lmu,sqrt(1/lsig)))

# parameter set
sala.parameters <- c("b0", "b1", "b2", "b3", "tau0", "tau1","sigma0","sigma1" )

# run WinBUGS (sala.bug) with 3 chains
post <- bugs(sala.data, sala.inits, sala.parameters, "sala.bug",
n.chains=3, n.iter=3000, n.burnin=2000,   # 1000 iterations after 2000 burn-in
n.thin=1, debug=F, bugs.directory="C:/Program Files/WinBUGS14/" )

# posterior samples
samp <- post$sims.matrix  # size 3000 x 6

\end{verbatim}

Note that in WinBUGS, \texttt{lsig} is  the inverse of the standard deviation of the log variable.
The WinBUGS code ``sala.bug'' is % the WinBUGS code to be run using the R function \texttt{bugs}.
is available in Appendix.
%The posterior samples of the six model parameters (four fixed effects and two log-variance parameters)
%are saved as \texttt{samp} of size $M \times p$, where $M$ is the posterior sample size and $p$ is the number of parameters.
%The object \texttt{samp} is included in the package. %One can use \texttt{samp} to test the package.
The results of the running code above are stored in the package as object \texttt{samp}.
One who do not want to run the code can use \texttt{samp}.
Note that any other methods or software can be used to obtain the posterior samples.


\subsection{Step 2: Obtain Parameter and Standard Error Estimates}

Once the posterior samples for model parameters are obtained,
\texttt{mcll\_est} can be used to obtain parameter estimates.
%\texttt{mcll} provides a function to compute standard errors.
%for parameter estimation,
\texttt{mcll\_est} uses a quadratic function (polynomial degree 2) and
a tricube function for the weight function.
A bandwidth is chosen at each data point
so that the local neighborhood contains a specified number of points.
Specifically, \texttt{mcll\_est} requires a smoothing parameter $\alpha$ between 0 and 1, which is the nearest neighbor bandwidth
with the $k$th smallest distance $d$ where $k = \lfloor n \alpha \rfloor $ and $d(x, x_i) = | x - x_i | $
with the sample size $n$. %A default value for $\alpha$  is 0.7.
Finally, a product kernel is used in (1) given the posterior samples (obtained in Step 1).

\texttt{mcll\_est} requires a prior function which returns the log prior densities for parameter values.
Specifically, the prior function should have as an argument a vector of parameter values ($vec.t$) and return value of
the log prior density for those parameter values $vec.t$.
For example, for the normal priors of the six parameters ($\beta_1$ to $\beta_4$ and $\tau_f$ and $\tau_m$) in model (2),
the prior function can be specified as
\begin{verbatim}
prior.func <- function(vec.t) {
    sum(dnorm(vec.t, m= c(0,0,0,0, -0.9870405, -0.9870405) ,
                sd=c(100,100,100,100, 1/0.766672, 1/0.766672) , log=T))
}
\end{verbatim}


Here is an example of using \texttt{mcll\_est}  to estimate parameters for model (2).

\begin{verbatim}
library(mcll)
# posterior samples
data(samp)
# prior function
prior.func <- function(vec.t) {
    sum(dnorm(vec.t, m= c(0,0,0,0, -0.9870405, -0.9870405) ,
                sd=c(100,100,100,100, 1/0.766672, 1/0.766672) , log=T))
}
## parameter estimation
run1 <- system.time(
    result1 <- mcll_est(data=samp, prior.func= prior.func, alp=0.7,
        method = "BFGS", control= list(maxit=10000), use.locfit=TRUE )
)
\end{verbatim}

If \texttt{use.locfit=TRUE}, the package \texttt{locfit} (Loader, 2012) is used to compute a local likelihood density estimate.
\texttt{locfit} tends to be faster but can fail for high-dimensional problems. % (e.g., $d >15$) and a manual option
In these cases, a version of the local likelihood code is implemented in the package (use \texttt{use.locfit=FALSE}) and
%with \texttt{use.locfit=FALSE} is recommended. If \texttt{se.locfit=FALSE}, optimization methods can be chosen.
optimization methods can be chosen for finding the polynomial coefficients.

\texttt{mcll\_est} returns the parameter estimates in the original scale
as well as the usual output from \texttt{optim}. % the function value, counts, convergence, and message are returned from \texttt{optim}.

\begin{verbatim}
result1
$par
            b0        b1         b2       b3       tau0     tau1
[1,] 0.9275766 -2.871686 -0.6488625 3.589313 0.08118962 0.148478

$convergence
[1] 0

$value
[1] -26.38284

$counts
function gradient
     112       15

$message
NULL

\end{verbatim}

\texttt{value} is the unnormalized log-likelihood returned from the MCLL algorithm.
It can be used to compute the Bayes factor. For more information on this, see Section 4 in Jeon et al. (2012). %\citep{jeon:12c}.
This parameter estimation  took about 9 seconds on a Intel Pentium Dual-Core 2.5-GHz processor computer with 3.2 GB of memory.

For standard error estimation, % standard errors of the parameter estimates,
the function \texttt{mcll\_se} is used.
%\begin{verbatim}
%mcll_se(data, par, H.prior, alp,
%        method="Nelder-Mead",  lower = -Inf, upper = Inf, control=list() )
%\end{verbatim}
%
%\noindent with the following arguments
%
%\begin{itemize}
%\item \textbf{data}: Posterior samples
%\item \textbf{par}: MCLL parameter estimates
%\item \textbf{H.prior}: Hessian matrix of the prior
%\item \textbf{method, lower, upper, control}: Optimization method and control options
%
%\end{itemize}
\texttt{mcll\_se} requires the Hessian matrix of the log prior \textbf{H.prior} evaluated at the MCLL parameter estimates.
% and \textbf{quad, low, up}.
One can solve it analytically if a closed-form solution is available.
For example, for the multivariate normal priors for the six parameters with zero mean and variance $p.var$,
the Hessian matrix can be obtained as
\begin{verbatim}
p.var = c(100,100,100,100, 1/0.766672, 1/0.766672)^2
H.prior <- -diag(1/p.var)
\end{verbatim}

Alternatively, one can use a numerical solution for the Hessian matrix using e.g.,
the \texttt{hessian} function in the \texttt{numDeriv} R package.

\begin{verbatim}
library(numDeriv)
log.prior.h <- hessian(prior.func, par)
\end{verbatim}

%Note that \texttt{par} is the parameter estimates in the original scale.
%For \texttt{optim}, a default choice for \texttt{mcll\_se} is Nelder-Mead.
Here is an example of using \texttt{mcll\_se} to compute standard errors for the parameter estimates for model (2).

\begin{verbatim}
run2 <- system.time(
    result2 <- mcll_se(data=samp, par=par, H.prior = H.prior,  alp=0.7,
        method= "Nelder-Mead" , control=list(maxit=20000) )
)
result2
       b0        b1        b2        b3      tau0      tau1
0.4057844 0.5640063 0.4907643 0.6663096 0.3022842 0.2999727
\end{verbatim}

\texttt{mcll\_se} returns a vector of standard errors for the estimates for the model parameters ($\beta_1$ to $\beta_4$, $\tau_f$ and $\tau_m$).
Standard error estimation took about 122 seconds on a Intel Pentium Dual-Core 2.5-GHz processor computer with 3.2 GB of memory.


\section{Discussion}

%@ future work/ extensions

There are several things to be discussed regarding implementation of MCLL.
First, we uses an orthogonal transformation of the posterior samples.
This orthogonal transformation, also called data presphering (Wand and Jones, 1993) %\citep{wand:93}
is useful in implementing MCLL because it simplifies the integral term in (1).
%to apply the product kernel in (\ref{eq:prodker}) in Step 2.
Specifically, for multidimensional parameter $\boldsymbol{\theta}$,
if the components are approximately independent in the posterior, then interactions terms in $P_{\boldsymbol{a}}(\uu-\boldsymbol{\theta})$
can be dropped. In addition,
a product kernel can be used, with  %assuming an orthogonal parameter space as
\begin{equation}\label{eq:prodker}
K \left(\frac{\uu - \boldsymbol{\theta}}{\hh}\right) = \prod_{i=1}^d K_0 \left(\frac{u_i - \theta_i }{h_i}\right),
\end{equation}
\noindent where $K_0$ is a one-dimensional kernel. With these two simplifications, the multidimensional integral
can be factorized as a product of one-dimensional integrals due to the orthogonality of the parameter space.
In addition, the orthogonal transformation standardizes
a bandwidth choice by transforming the parameter space to be on the same scale.
That is, the default choice for $\alpha=0.7$ works in most applications.

Second,
we use a log-transformation of variance parameters.
This has several advantages: 1) it avoids need for a modified kernel in Step 2 to handle truncation of the density at zero,
2) the posterior distributions are closer to normal so that data presphering operation works better for a symmetric distribution,
and 3)the log-posterior is better approximated by a quadratic function.


Third,
for model (2), we used diffuse priors for the fixed and log standard deviation parameters.
In this case, we have shown that the posterior mean estimates as well as MCLL estimates
are also close to ML estimates (Jeon et al., 2012, Section 5). %\citep[Section.5]{jeon:12c}. % as well as MCLL estimates.
Note that even if priors are poorly specified, %however,
the MCLL algorithm provides results close to the ML estimates while the posterior mean estimates are not.
%Informative priors are useful for improving mixing in MCMC in Step 1 but some care is required.
For details, see Section 6.2 in Jeon et al. (2012). %\citet{jeon:12c}.

Finally,
it is important to note that MCLL allows likelihood inference for any complex models
for which ML estimation may be infeasible but MCMC methods are possible.
For example, in addition to GLMMs with crossed random effects considered here,
the MCLL algorithm could be used to fit models with higher dimensional latent variables such as
spatial models for disease mapping.
%We have shown that the MCLL method provides results close to the ML estimates. % than the Bayesian estimates.
%Even if informative priors are specified, MCLL provides estimates close to the ML estimates,
%whereas the posterior mean estimates could be quite different. %rather should be seen as Bayesian estimates.
Therefore, when ML inference is desirable for highly complex models,
the MCLL method is an effective and practical choice.


\section*{Appendix}

Here is the WinBUGS code for model (2) for the salamander mating data.
To use this for \texttt{bugs}, save this as ``sala.bug'' as shown in Section 2.1.

\begin{verbatim}
## crossed random effects model for salamander data

# n: number of salamanders
# m: number of female
# f: number of male
# rm: random effects for male
# rf: random effects for female
# mu0: mean for male
# mu1: mean for female
# zeta0: inverse variance for male
# zeta1: inverse variance for female

model
{

for (i in 1:n) {
        logit(p[i]) <- b0 + b1*wsf[i] + b2*wsm[i] + b3*ww[i]
        + rm[male[i]] + rf[female[i]]
        r[i] ~ dbern(p[i])
}

b0 ~ dnorm(0, .0001)
b1 ~ dnorm(0, .0001)
b2 ~ dnorm(0, .0001)
b3 ~ dnorm(0, .0001)

for (j in 1:m) {
    rm[j] ~ dnorm(0,zeta0)
}

zeta0 <- pow(exp(tau0),-2)
tau0 ~ dnorm(lmu,lsig) # normal
sigma0 <- exp(tau0)

for (k in 1:f) {
    rf[k] ~ dnorm(0,zeta1)
}

zeta1 <- pow(exp(tau1),-2)
tau1 ~ dnorm(lmu,lsig)
sigma1 <- exp(tau1)

}

\end{verbatim}


\begin{thebibliography}{10}

\bibitem{}
Gelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences.
Statistical Science 7, 457-472.

\bibitem{}
Jeon, M., Kaufman, C., and Rabe-Hesketh, S. (2012). Monte Carlo local likelihood for estimating
generalized linear mixed models. Submitted for publication.

\bibitem{}
Karim, M. and Zeger, S. (1992). Generalized linear models with random effects: Salamander mating
revisited. Biometrics 48, 631-644.

\bibitem{}
Loader, C. (1996). Local likelihood density estimation. The Annals of Statistics 24, 1602-1618.

\bibitem{}
Loader, C. (2012). locfit: local regression, likelihood, and density estimation. Downloadable from
http://cran.r-project.org/web/packages/locfit/index.html.

\bibitem
Lunn, D., Thomas, A., Best, N., and Spiegelhalter, D. (2000). WinBUGS - a Bayesian modelling
framework: concepts, structure, and extensibility. Statistics and Computing 10, 325-337.

\bibitem{}
McCullagh, P. and Nelder, J. (1989). Generalized Linear Models. Chapman and Hall, New York.

\bibitem{}
Sturtz, S., Ligges, U., and Gelman, A. (2005). R2WinBUGS: A package for running WinBUGS
from R. Journal of Statistical Software 12, 1-16.

\bibitem{}
Wand, M. P. and Jones, M. C. (1993). Comparison of smoothing parameterizations in bivariate
kernel density estimation. Journal of the American Statistical Association 88, 520-528.


\end{thebibliography}


\end{document}


